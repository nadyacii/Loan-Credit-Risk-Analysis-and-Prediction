# -*- coding: utf-8 -*-
"""Final_Task_IDX_Partner_Data_Scientist_Nadia_Chusnul_Ikromah (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1feMwd8FYKLWx5caGF4AP2lmdEzgPox1W

# LOAN CREDIT RISK PREDICTION

Author: Nadia Chusnul Ikromah

## IMPORT PACKAGES
"""

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# import numpy and pandas for data manipulating and data analysis
import pandas as pd
import numpy as np

# import matplotlib and seaborn for plotting
import matplotlib.pyplot as plt
from datetime import datetime
import seaborn as sns

#import scikit learn for modelling
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

#scikit learn for model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, auc, roc_curve
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold

# scikit learn for categorical variables
from sklearn.preprocessing import LabelEncoder

# scikit learn for scaling
from sklearn.preprocessing import StandardScaler, RobustScaler

# file system management
import os

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/loan_data_2007_2014.csv")

"""## DATA UNDERSTANDING

### Identify the Structure of the Dataset
"""

# Show the first 5 rows of the dataframe
df.head()

# Show all column names in the DataFrame
df.columns

# Print the number of columns and rows in the DataFrame
print("Data contains %.f columns and %.f rows" %(df.shape[1], df.shape[0]))

# Print the number of duplicated rows in the DataFrame
print('This data contains %.f duplicated data' %df.duplicated().sum())

# Compute summary statistics for numerical columns
numerical_summary = df.describe().transpose()

# Define color palette
palette = sns.color_palette("viridis", as_cmap=True)

# Define a function to apply background gradient
def apply_gradient(s):
    return [f'background-color: {palette(s)}' for s in s]

# Apply background gradient using Styler.apply()
styled_summary = numerical_summary.style.apply(apply_gradient)

# Show the styled summary statistics
styled_summary

# Show the count of unique values in the 'loan_status' column
df.loan_status.value_counts()

"""The loan_status variable contains several categories:

- Current: The borrower is making payments on time.

- Charged Off: The loan has been written off due to non-payment.

- Late: The borrower has missed a payment.

- In Grace Period: The borrower is within the allowed grace period before a late fee is applied.

- Fully Paid: The loan has been completely repaid.

- Default: The borrower has stopped making payments.

Based on these definitions, each borrower can be classified as either a good loaner or a bad loaner.

To simplify the classification, I will categorize borrowers as bad loaners if they have missed payments for more than 30 days or have a worse status. Borrowers outside this category will be considered good loaners.









"""

# Create a mapping dictionary to categorize loan status as 'good loaner' or 'bad loaner'
mapping = {
    'Current': 'good loaner',
    'Fully Paid': 'good loaner',
    'Charged Off': 'bad loaner',
    'Late (31-120 days)': 'bad loaner',
    'In Grace Period': 'bad loaner',
    'Does not meet the credit policy. Status:Fully Paid': 'good loaner',
    'Late (16-30 days)': 'good loaner',
    'Default': 'bad loaner',
    'Does not meet the credit policy. Status:Charged Off': 'bad loaner'
}

# Replace the values in the 'loan_status' column based on the mapping
df['loan_status'] = df['loan_status'].replace(mapping)

# Set the figure size for better visualization
plt.figure(figsize=(16,8))

# Define labels and colors for the pie chart
labels = ['Good loaner', 'Bad loaner']
colors = ['#A8E6CF', '#FF8A80']  # Warna baru
explode = [0, 0.1]

# Count the occurrences of each loan status category
loan_counts = df['loan_status'].value_counts()

# Create a pie chart to visualize loan status distribution
plt.pie(loan_counts, labels=labels, autopct='%1.1f%%', startangle=90, shadow=False, colors=colors, explode=explode, textprops={'fontsize': 16}, wedgeprops={'linewidth': 2, 'edgecolor': 'white'})
centre_circle = plt.Circle((0,0), 0.70, fc='white')
plt.gca().add_artist(centre_circle)

# Set the title and formatting
plt.title('Loan Status', fontsize=15)
plt.gca().set_aspect('equal')
plt.legend(title='Loan Status')

# Show the chart
plt.show()

"""The chart shows that 88.4% of borrowers are good loaners, while 11.6% are bad loaners. This indicates that most applicants repay their loans on time, but a small percentage pose a financial risk.

Who applies for credit?
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all text data from the 'emp_title' column into a single string
title_loan = " ".join(str(tujuan) for tujuan in df['emp_title'].dropna())

# Generate a WordCloud visualization
word_cloud = WordCloud(
    collocations=False,
    background_color='white',
    width=2048, height=1080,
    colormap='Set1'
).generate(title_loan)

# Show WordCloud
plt.figure(figsize=(12, 12))
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.title("Who Applies Credit?", fontsize=18, fontweight='bold')
plt.show()

"""The word cloud shows the most common job titles of individuals applying for credit. The largest words, such as Manager, Assistant, Teacher, Director, Service, and Specialist, indicate that professionals from various industries, including management, education, healthcare, and technical fields, frequently seek loans. This suggests that credit applicants come from diverse backgrounds, with a significant number in leadership and service-oriented roles.

Where are the borrowers domiciled?
"""

# Sort the data by the number of borrowers in each state
state_counts = df['addr_state'].value_counts()
colors = sns.color_palette("Blues", len(state_counts))[::-1]
plt.figure(figsize=(12, 8))
sns.barplot(x=state_counts.values, y=state_counts.index, palette=colors)
plt.title('Where Are the Borrowers Domiciled?', fontsize=18, fontweight='bold')
plt.xlabel('Number of Borrowers', fontsize=14)
plt.ylabel('State', fontsize=14)
plt.grid(axis='x', linestyle='--', alpha=0.7)

# Show the plot
plt.show()

"""The bar chart shows the number of borrowers by state, with California (CA), New York (NY), Texas (TX), and Florida (FL) having the highest number of borrowers. This suggests that loan applications are more concentrated in highly populated and economically active states.

Why did borrowers apply for loans?
"""

# Get the loan purpose categories sorted by frequency in descending order
order = df['purpose'].value_counts(ascending=False).index
plt.figure(figsize=(15,10))
ax = sns.countplot(y='purpose', data=df, orient='v', order=order, color='#8CC0DE')
plt.title('Why did borrowers apply for loans?', fontsize=20)
plt.yticks(size=16)
plt.xticks(size=16)
ax.set_ylabel("Purpose",fontsize=16)
ax.set_xlabel("Proportion of Customers",fontsize=16)

"""Based on the visualization above, the majority of borrowers applied for loans for debt consolidation and credit card purposes. This suggests that many borrowers are using loans to manage existing debts."""

# Create a copy of the original DataFrame to preserve the original data
df2=df.copy()

"""## EXPLORATORY DATA ANALYSIS"""

# Show DataFrame summary
df.info()

# Show the first 5 rows of the dataset
df.head()

"""Check Categorical Feature"""

# Count the number of unique values in categorical column
df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)

"""### Missing Value"""

# Check the percentage of missing values in each column
missing_values = df.isnull().sum() / len(df)
missing_values = missing_values[missing_values > 0]
missing_values.sort_values(ascending=False, inplace=True)
missing_values

"""The data has 40 columns with missing values, which means some information is incomplete. This could affect analysis and decision-making. To fix this, we need to check why the data is missing and decide the best way to handle it, such as filling in missing values, removing incomplete data, or using other methods to ensure accurate results."""

# Filter columns with more than 50% missing values
missing_values = missing_values[missing_values > 0.50]
missing_values_sorted = missing_values.sort_values(ascending=False)
print(missing_values_sorted)

"""There are 21 columns with more than 50% missing values in the dataset. To keep the data clean and reliable, these columns were removed. This helps make the analysis more accurate and avoids problems caused by too much missing data."""

# Drop columns that have more than 50% missing values
df = df.drop(columns=missing_values_sorted.index)

# Show the first 5 rows of the dataset
df.info()

"""There are still some columns in the dataset that do not significantly contribute to answering the desired objective. Therefore, these columns will be removed. Some of them include:

- Unnamed: 0
- id
- member_id
- url
- grade
- emp_title
- sub_grade
- home_ownership
- verification_status
- purpose
- pymnt_plan
- title
- zip_code
- addr_state
- policy_code
- application_type
- initial_list_status
- funded_amnt_inv
- total_pymnt
- total_pymnt_inv
"""

# Define a list of columns to be dropped
drop_columns = [
    'Unnamed: 0', 'id', 'member_id', 'url', 'grade','emp_title', 'sub_grade', 'home_ownership', 'verification_status', 'purpose',
    'pymnt_plan', 'title', 'zip_code', 'addr_state', 'policy_code',
    'application_type', 'initial_list_status', 'funded_amnt_inv',
    'total_pymnt', 'total_pymnt_inv'
]

# Remove drop_columns from the DataFrame
df = df.drop(columns=drop_columns)

# Show the number of rows and columns in the DataFrame
df.shape

# Show the names of all columns in the DataFrame
df.columns

# Count the number of missing values in each column
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values > 0]
print(missing_values)

"""Based on the data, there are still some columns that contain missing values. Below are the columns with missing data that will be removed along with the reasons:

- next_pymnt_d: Only applies to active loans, making it irrelevant for prediction.

- last_pymnt_d: Has the potential to cause data leakage.

- last_credit_pull_d: Does not have a significant impact on the model being developed.

- tot_coll_amt: Contains too many missing values, making imputation unfeasible.

- tot_cur_bal: Contains too many missing values, making imputation unfeasible.

- total_rev_hi_lim: Contains too many missing values, making imputation unfeasible.

Next, here are the columns with missing values that are considered to be kept since imputation is still possible, along with the reasons:

- emp_length: Missing values can be filled with "Unknown" since it is categorical.

- revol_util: Missing values can be filled with the median value, as it represents a percentage of credit utilization.

- collections_12_mths_ex_med: Missing values can be filled with 0 based on the majority of the data.









"""

# Remove rows with missing values in selected columns
df = df.dropna(subset=[
    'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim'
])

# Impute missing values for specific columns
df['emp_length'].fillna('Unknown', inplace=True)
df['revol_util'].fillna(df['revol_util'].median(), inplace=True)
df['collections_12_mths_ex_med'].fillna(0, inplace=True)

# Check if there are any missing values left in the dataset
print(df.isnull().sum().sum())

# Show the first 5 rows of the dataset
df.head()

"""From the data preview above, the next step in data cleaning will focus on the following columns:

- term
- emp_length
- issue_d
- earliest_cr_line
- last_pymnt_d
- next_pymnt_d
- last_credit_pull_d

Cleaning the `term` Column
"""

# Check unique values in the 'term' column

df['term'].unique()

"""It can be seen that there are two types of data: '36 months' and '60 months'. Since the data is still in string format, we will convert it to integers."""

# Remove the string " months" and convert the 'term' column to integer type
df['term'] = df['term'].str.replace(" months", "").astype(int)

# Check unique values in the 'term' column after conversion
df['term'].unique()

"""Cleaning the `emp_length` Column"""

# Check unique values in the 'emp_length' column
df['emp_length'].unique()

# Check how many times each value appears in the 'emp_length' column
df['emp_length'].value_counts()

"""It can be seen that there are several categorical data types in the `emp_length` column. We will convert this data into integers. Additionally, the 'Unknown' data will be removed since its quantity is small compared to other data."""

# Convert employment length to numbers
df['emp_length'] = df['emp_length'].replace({
    '10+ years': 10,
    '9 years': 9,
    '8 years': 8,
    '7 years': 7,
    '6 years': 6,
    '5 years': 5,
    '4 years': 4,
    '3 years': 3,
    '2 years': 2,
    '1 year': 1,
    '< 1 year': 0
})

# Remove rows where 'emp_length' is 'Unknown'
df = df[df['emp_length'] != 'Unknown']

# Change 'emp_length' to integer type
df['emp_length'] = df['emp_length'].astype(int)

# Show unique values in the 'emp_length' column after conversion
df['emp_length'].unique()

"""Cleaning the `issue_d`, `earliest_cr_line`, `last_pymnt_d`, `next_pymnt_d`, and `last_credit_pull_d` Column"""

# Convert selected columns to datetime format
date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d']

for col in date_columns:
  df[col] = pd.to_datetime(df[col], format='%b-%y', errors='coerce')

# Show the first 5 rows of the date columns
df[date_columns].head()

"""### Correlation Matrix"""

# Show summary information about the DataFrame
df.info()

# Show the first five rows of the dataset
df.head()

# Show statistical summary of numerical columns, excluding object and datetime columns
df.describe(exclude=['object', 'datetime64'])

"""Creating a New Column"""

# Check unique values in the 'loan_status' column

df['loan_status'].unique()

# Function to categorize loan_status into numerical values

def categorize_loan_status(status):
  if status in ['good loaner']:
    return 0
  elif status in ['bad loaner']:
    return 1
  else:
    return None

"""The categorize_loan_status function classifies loan applicants into two categories:

- Good loaners are labeled as 0

- Bad loaners are labeled as 1

- Other statuses are assigned None

This function helps in identifying risky borrowers and making better loan approval decisions.
"""

# Apply the categorization function to create a new column 'good_bad_loaner'
df['good_bad_loaner'] = df['loan_status'].apply(categorize_loan_status)

# Remove rows where 'good_bad_loaner' has None values
df = df.dropna(subset=['good_bad_loaner'])

# Convert 'good_bad_loaner' column to integer type
df['good_bad_loaner'] = df['good_bad_loaner'].astype(int)

# Count the number of good and bad loaners
df['good_bad_loaner'].value_counts()

"""Exploring Column Relationships Using a Heatmap"""

# Calculate and visualize the correlation between numerical features
numeric_cols = df.select_dtypes(include=['number']).columns
correlation_matrix = df[numeric_cols].corr()

fig, ax = plt.subplots(figsize=(18, 14))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap=cmap, linewidths=0.75, square=True, ax=ax)

title = "Correlation Matrix"
plt.title(title, fontsize=16, fontweight='bold')
plt.show()

"""The correlation matrix reveals several key relationships between features. The loan amount and funded amount have a perfect correlation (1.00), indicating they are nearly identical. Similarly, installment and loan amount show a strong correlation (0.95), suggesting that higher loans result in higher monthly payments. Additionally, out_prncp and out_prncp_inv also have a perfect correlation (1.00), as both represent the outstanding principal. Meanwhile, the good/bad loaner label shows weak correlations with other features, indicating that loan default risk is influenced by multiple factors beyond those captured in this dataset. These insights are crucial for refining feature selection in predictive modeling.

Exploring Numerical Data Distribution with Histograms
"""

# Visualizing the distribution of all numerical features using histograms
numeric_cols = df.select_dtypes(include=['number']).columns
plt.figure(figsize=(20, 15))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(-(-len(numeric_cols) // 4), 4, i)
    sns.histplot(df[col], bins=30, kde=True, color='lightblue')
    plt.title(col, fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

"""Based on the histogram, the columns `int_rate` and `dti` exhibit a distribution that is close to normal. This suggests that these features may follow a typical bell-shaped curve, which could be useful for statistical analysis and modeling.

Outlier Analysis for Numerical Data
"""

# Visualizing outliers in numerical features using boxplots
numeric_cols = df.select_dtypes(include=['number']).columns
plt.figure(figsize=(23, 20))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(-(-len(numeric_cols) // 4), 4, i)
    sns.boxplot(y=df[col], color='lightblue', width=0.5)
    plt.title(col, fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

"""Univariate Analysis"""

# Visualizing the distribution of selected key features
features = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'revol_util', 'good_bad_loaner']
plt.figure(figsize=(15, 10))

for i, col in enumerate(features, 1):
    plt.subplot(3, 2, i)
    sns.histplot(df[col], bins=30, kde=True, color='lightblue')
    plt.title(f'Distribution of {col}')
    plt.xlabel('')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""Bivariate Analysis"""

# Visualizing the average interest rate for good and bad loaners
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=df['good_bad_loaner'], y=df['int_rate'], palette=['lightblue'])

for p in ax.patches:
    ax.annotate(f"{p.get_height():.2f}",
                (p.get_x() + p.get_width() / 2, p.get_height()),
                ha='center', va='bottom', fontsize=12, color='black')

plt.title("Average Interest Rate Based on Good/Bad Loaner", fontsize=14)
plt.xlabel("Good/Bad Loaner", fontsize=12)
plt.ylabel("Average Interest Rate (%)", fontsize=12)
plt.show()

"""## DATA PREPARATION

### Handling Outliers
"""

# Visualizing outliers in selected numerical columns using boxplots
outlier_columns = [
    'loan_amnt', 'term', 'int_rate', 'installment', 'emp_length',
    'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'revol_util',
    'total_acc', 'total_rec_late_fee', 'collections_12_mths_ex_med'
]

plt.figure(figsize=(15, 10))

for i, col in enumerate(outlier_columns):
    plt.subplot((len(outlier_columns) // 4) + 1, 4, i + 1)
    sns.boxplot(y=df[col], color="lightblue")
    plt.title(col)

plt.tight_layout()
plt.show()

"""Based on the boxplot above, several columns contain extreme outliers. To reduce these outliers, a transformation will be applied to minimize their impact."""

# Displaying summary statistics for columns that may contain outliers
df[outlier_columns].describe()

# Applying log transformation to selected features
features_to_log = [
    'annual_inc', 'total_rec_late_fee', 'delinq_2yrs', 'collections_12_mths_ex_med', 'inq_last_6mths'
]

df[features_to_log] = df[features_to_log].transform(np.log1p)

# Visualizing the distribution after log transformation
plt.figure(figsize=(15, 10))
for i, col in enumerate(features_to_log):
  plt.subplot(len(features_to_log) // 2 + 1, 2, i + 1)
  sns.histplot(df[col], bins=30, kde=True)
  plt.title(f'Distribution of {col} after Log Transformation')

plt.tight_layout()
plt.show()

"""With the application of Log Transformation, the distribution of the `annual_inc` column appears more normal, making it ready to be used as a feature in the model. However, some columns, such as `total_rec_late_fee`, `collections_12_mths_ex_med`, `delinq_2yrs`, and `inq_last_6mths`, still have high peaks, likely due to many borrowers having no late payments or outstanding collections."""

# Confirming that the transformed features are correctly assigned to the main dataframe
for col in features_to_log:
  df[col] = df[col]

# Show all column names in the dataframe
df.columns

"""### Scaling Data"""

# Scaling numerical features using StandardScaler
features_to_scale = [
    'loan_amnt', 'term', 'int_rate', 'installment', 'emp_length',
    'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'revol_util',
    'total_acc', 'total_rec_late_fee', 'collections_12_mths_ex_med'
]

scaler = StandardScaler()

df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

df[features_to_scale].describe().T

# Making sure the scaled features remain in the dataframe after transformation.
for col in features_to_scale:
  df[col] = df[col]

"""Selection Feature"""

# Selecting relevant features for analysis and modeling,
selected_features = [
    'loan_amnt', 'term', 'int_rate', 'installment', 'emp_length',
    'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'revol_util',
    'total_acc', 'total_rec_late_fee', 'collections_12_mths_ex_med', 'good_bad_loaner'
]

df = df[selected_features]

# Check the features in the dataset

print(df.head())
print("Remaining columns: ", df.columns)

"""SMOTE for Imbalance Data"""

from imblearn.over_sampling import SMOTE
from collections import Counter

# Split the features and target
X = df.drop(columns=['good_bad_loaner']) # Features
y = df['good_bad_loaner'] # Target

# Apply SMOTE for oversampling the minority class
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Check the distribution before applying SMOTE
print("Before Oversampling:", Counter(y))

# Check the distribution after applying SMOTE
print("After Oversampling:", Counter(y_resampled))

from collections import Counter
import matplotlib.pyplot as plt

# Visualizing the distribution of the target variable before and after applying SMOTE using donut charts
before_counts = Counter(y)
after_counts = Counter(y_resampled)
colors = ['#4CAF50', '#FF5722']
labels = ['Good Loaner', 'Bad Loaner']

# Create subplots for donut charts
fig, axes = plt.subplots(1, 2, figsize=(15, 7))

# Donut chart before SMOTE
axes[0].pie(before_counts.values(), labels=labels, colors=colors, autopct='%1.1f%%',
            pctdistance=0.85, explode=[0.05, 0.05], textprops={'fontsize': 14},
            wedgeprops={'edgecolor': 'white'})
axes[0].add_artist(plt.Circle((0, 0), 0.70, fc='white'))
axes[0].set_title('Target Distribution before SMOTE', fontsize=16)

# Donut chart after SMOTE
axes[1].pie(after_counts.values(), labels=labels, colors=colors, autopct='%1.1f%%',
            pctdistance=0.85, explode=[0.05, 0.05], textprops={'fontsize': 14},
            wedgeprops={'edgecolor': 'white'})
axes[1].add_artist(plt.Circle((0, 0), 0.70, fc='white'))
axes[1].set_title('Target Distribuation after SMOTE', fontsize=16)

# Display the charts
plt.tight_layout()
plt.show()

"""### Spliting Data"""

# Splitting the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# Display the number of samples in the training and testing datasets
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

# Visualize the 'good_bad_loaner' column using a Barplot

sns.countplot(x=y_test)
plt.title("Distribution of Loan Status")
plt.show()

"""## DATA MODELLING"""

#Logistic Regression
lr = LogisticRegression(penalty='l2', random_state=42)
lr.fit(X_train, y_train)
Y_pred = lr.predict(X_test)

# Metrics
acc_lr = round(accuracy_score(y_test, Y_pred) * 100, 2)
prec_lr = round(precision_score(y_test, Y_pred) * 100, 2)
recall_lr = round(recall_score(y_test, Y_pred) * 100, 2)
roc_auc_lr = round(roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]) * 100, 2)
cv_lr = round(cross_val_score(lr, X_test, y_test, cv=3).mean() * 100, 2)

# Print results
print(f"Accuracy: {acc_lr}")
print(f"Precision: {prec_lr}")
print(f"Recall: {recall_lr}")
print(f"ROC-AUC: {roc_auc_lr}")
print(f"Cross Validation Score: {cv_lr}")

# Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
rf.fit(X_train, y_train)
Y_pred = rf.predict(X_test)

# Metrics
acc_rf = round(accuracy_score(y_test, Y_pred) * 100, 2)
prec_rf = round(precision_score(y_test, Y_pred) * 100, 2)
recall_rf = round(recall_score(y_test, Y_pred) * 100, 2)
roc_auc_rf = round(roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]) * 100, 2)
cv_rf = round(cross_val_score(rf, X_test, y_test, cv=3).mean() * 100, 2)

# Print results
print(f"Accuracy: {acc_rf}")
print(f"Precision: {prec_rf}")
print(f"Recall: {recall_rf}")
print(f"ROC-AUC: {roc_auc_rf}")
print(f"Cross Validation Score: {cv_rf}")

# Decision Tree
dt = DecisionTreeClassifier(max_depth=4, random_state=42)
dt.fit(X_train, y_train)
Y_pred = dt.predict(X_test)

# Metrics
acc_dt = round(accuracy_score(y_test, Y_pred) * 100, 2)
prec_dt = round(precision_score(y_test, Y_pred) * 100, 2)
recall_dt = round(recall_score(y_test, Y_pred) * 100, 2)
roc_auc_dt = round(roc_auc_score(y_test, dt.predict_proba(X_test)[:, 1]) * 100, 2)
cv_dt = round(cross_val_score(dt, X_test, y_test, cv=3).mean() * 100, 2)

# Print results
print(f"Accuracy: {acc_dt}")
print(f"Precision: {prec_dt}")
print(f"Recall: {recall_dt}")
print(f"ROC-AUC: {roc_auc_dt}")
print(f"Cross Validation Score: {cv_dt}")

# K-Nearest Neighbors
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
Y_pred = knn.predict(X_test)

# Metrics
acc_knn = round(accuracy_score(y_test, Y_pred) * 100, 2)
prec_knn = round(precision_score(y_test, Y_pred) * 100, 2)
recall_knn = round(recall_score(y_test, Y_pred) * 100, 2)
roc_auc_knn = round(roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1]) * 100, 2)
cv_knn = round(cross_val_score(knn, X_test, y_test, cv=3).mean() * 100, 2)

# Print results
print(f"Accuracy: {acc_knn}")
print(f"Precision: {prec_knn}")
print(f"Recall: {recall_knn}")
print(f"ROC-AUC: {roc_auc_knn}")
print(f"Cross Validation Score: {cv_knn}")

# Naïve Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)
Y_pred = nb.predict(X_test)

# Metrics
acc_nb = round(accuracy_score(y_test, Y_pred) * 100, 2)
prec_nb = round(precision_score(y_test, Y_pred) * 100, 2)
recall_nb = round(recall_score(y_test, Y_pred) * 100, 2)
roc_auc_nb = round(roc_auc_score(y_test, nb.predict_proba(X_test)[:, 1]) * 100, 2)
cv_nb = round(cross_val_score(nb, X_test, y_test, cv=3).mean() * 100, 2)

# Pribt results
print(f"Accuracy: {acc_nb}")
print(f"Precision: {prec_nb}")
print(f"Recall: {recall_nb}")
print(f"ROC-AUC: {roc_auc_nb}")
print(f"Cross Validation Score: {cv_nb}")

# Stochastic Gradient Descent (SGD)
sgd = SGDClassifier(loss='log_loss', penalty='l2', max_iter=1000, random_state=42)
sgd.fit(X_train, y_train)
Y_pred = sgd.predict(X_test)

# Metrics
acc_sgd = round(accuracy_score(y_test, Y_pred) * 100, 2)
prec_sgd = round(precision_score(y_test, Y_pred) * 100, 2)
recall_sgd = round(recall_score(y_test, Y_pred) * 100, 2)
roc_auc_sgd = round(roc_auc_score(y_test, sgd.decision_function(X_test)) * 100, 2)
cv_sgd = round(cross_val_score(sgd, X_test, y_test, cv=3).mean() * 100, 2)

# Print results
print(f"Accuracy: {acc_sgd}")
print(f"Precision: {prec_sgd}")
print(f"Recall: {recall_sgd}")
print(f"ROC-AUC: {roc_auc_sgd}")
print(f"Cross Validation Score: {cv_sgd}")

"""## MODEL EVALUATION"""

# Create a DataFrame to compare model performance based on accuracy, ROC-AUC, and cross-validation scores
models = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest',
              'Decision Tree', 'K-Nearest Neighbors (KNN)',
              'Gaussian Naive Bayes', 'Stochastic Gradient Descent'],
    'Score': [acc_lr,acc_rf, acc_dt, acc_knn,
              acc_nb, acc_sgd],
    'ROC-AUC': [roc_auc_lr, roc_auc_rf, roc_auc_dt, roc_auc_knn, roc_auc_nb, roc_auc_sgd],
    'Cross Validation': [cv_lr, cv_rf, cv_dt, cv_knn,
              cv_nb, cv_sgd]})

# Sort models based on Score, ROC-AUC, and Cross Validation in descending order
evaluation = models.sort_values(by=['Score','ROC-AUC','Cross Validation'], ascending=False)

# Show the evaluation table
evaluation

"""The K-Nearest Neighbors (KNN) model performs the best compared to other models. It has the highest Score (88.78), ROC-AUC (96.18), and Cross-Validation Score (78.14). This means KNN is very good at making predictions and distinguishing between classes."""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Visualizing confusion matrices for each classification model
models = {
    "Logistic Regression": LogisticRegression(penalty='l2', random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42),
    "Decision Tree": DecisionTreeClassifier(max_depth=4, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Naïve Bayes": GaussianNB(),
    "SGD": SGDClassifier(loss='log_loss', penalty='l2', max_iter=1000, random_state=42)
}

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle("Confusion Matrix", fontsize=20, fontweight='bold')

for ax, (model_name, model) in zip(axes.flatten(), models.items()):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Good (0)", "Bad (1)"],
                yticklabels=["Good (0)", "Bad (1)"], ax=ax)

    ax.set_title(model_name, fontsize=14)
    ax.set_xlabel("Predicted Label")
    ax.set_ylabel("True Label")


plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""### VALIDATION"""

# Predicting probabilities for the positive class using KNN
y_pred_proba = knn.predict_proba(X_test)[:][:,1]

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)
df_actual_predicted.index = y_test.index

# Plotting the ROC curve to evaluate the model's performance and calculating the AUC score.
fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

#classification reports and ROC AUC scores to evaluate model performance

from sklearn.metrics import classification_report, roc_auc_score

# Logistic Regression
y_pred_lr = lr.predict(X_test)
roc_auc_lr = round(roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]), 2)
report_lr = classification_report(y_test, y_pred_lr, output_dict=True)
print("Classification Report - Logistic Regression")
print(classification_report(y_test, y_pred_lr))
print(f"ROC AUC Score: {roc_auc_lr:.2f}\n")

# Random Forest
y_pred_rf = rf.predict(X_test)
roc_auc_rf = round(roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]), 2)
report_rf = classification_report(y_test, y_pred_rf, output_dict=True)
print("Classification Report - Random Forest")
print(classification_report(y_test, y_pred_rf))
print(f"ROC AUC Score: {roc_auc_rf:.2f}\n")

# Decision Tree
y_pred_dt = dt.predict(X_test)
roc_auc_dt = round(roc_auc_score(y_test, dt.predict_proba(X_test)[:, 1]), 2)
report_dt = classification_report(y_test, y_pred_dt, output_dict=True)
print("Classification Report - Decision Tree")
print(classification_report(y_test, y_pred_dt))
print(f"ROC AUC Score: {roc_auc_dt:.2f}\n")

# K-Nearest Neighbors
y_pred_knn = knn.predict(X_test)
roc_auc_knn = round(roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1]), 2)
report_knn = classification_report(y_test, y_pred_knn, output_dict=True)
print("Classification Report - K-Nearest Neighbors")
print(classification_report(y_test, y_pred_knn))
print(f"ROC AUC Score: {roc_auc_knn:.2f}\n")

# Naïve Bayes
y_pred_nb = nb.predict(X_test)
roc_auc_nb = round(roc_auc_score(y_test, nb.predict_proba(X_test)[:, 1]), 2)
report_nb = classification_report(y_test, y_pred_nb, output_dict=True)
print("Classification Report - Naïve Bayes")
print(classification_report(y_test, y_pred_nb))
print(f"ROC AUC Score: {roc_auc_nb:.2f}\n")

# SGD Classifier
y_pred_sgd = sgd.predict(X_test)
roc_auc_sgd = round(roc_auc_score(y_test, sgd.decision_function(X_test)), 2)
report_sgd = classification_report(y_test, y_pred_sgd, output_dict=True)
print("Classification Report - SGD Classifier")
print(classification_report(y_test, y_pred_sgd))
print(f"ROC AUC Score: {roc_auc_sgd:.2f}\n")

"""# Conclusion

1. Best model: K-Nearest Neighbors
2. The ROC curve demonstrates that the KNN model achieves an outstanding AUC (Area Under the Curve) score of 0.9618, indicating exceptional performance in distinguishing between good loans (0) and bad loans (1). This high AUC value means the model is highly effective at correctly identifying borrowers who are likely to default (true positives) while minimizing false alarms (false positives). Such strong predictive capability is critical for financial institutions to mitigate risks and optimize lending decisions.
"""